{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiments solely focuses on handling entity embeddings from glove datasets with only numpys, sklearn and scipy. I wanted to craft networks from scratch rather than using pytorch (like I usually do and didn't want to explode my pc again)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3fe710-fcbb-4e24-a1b1-1eb133374509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering basement now. Fetching books . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimiphan/Library/Mobile Documents/com~apple~CloudDocs/Documents/MBA Documents/MyPlayground/honeycombs/gloveutils.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2452232281d946ab9d84ae7e88288b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N.o of books loaded in shelf! There are 400001 books on the shelves\n"
     ]
    }
   ],
   "source": [
    "\n",
    "names = [\n",
    "    \"Alice\",\n",
    "    \"Bob\",\n",
    "    \"Charlie\",\n",
    "    \"David\",\n",
    "    \"Emma\",\n",
    "    \"Fiona\",\n",
    "    \"George\",\n",
    "    \"Hannah\",\n",
    "    \"Isabella\",\n",
    "    \"Jack\"\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "from gloveutils import GloveBox \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "class Profiles:\n",
    "    def __init__(self, names: list, **kwargs: dict):\n",
    "        self.username = names\n",
    "        self.friends = kwargs['embedding'] # size=[num of entities, 50]\n",
    "        \n",
    "        assert isinstance(self.friends, np.ndarray), f'Expected np type, but received {type(self.friends)}'\n",
    "        assert self.friends.shape[-1] == 50, f\"Glove embedding has unusual shape: {self.friends.shape}\"\n",
    "        \n",
    "    def location(self):\n",
    "        phone = cosine_similarity(self.friends, self.friends)\n",
    "        assert phone.shape[0] == phone.shape[-1], f'Expected N by N matrix but received: {phone.shape}'\n",
    "        data = pd.DataFrame(phone, columns=self.username, index=self.username)\n",
    "        print(data)\n",
    "\n",
    "book = GloveBox()\n",
    "book.enter_basement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.16386 ,  0.57795 , -0.59197 , -0.32446 ,  0.29762 ,  0.85151 ,\n",
       "        -0.76695 , -0.20733 ,  0.21491 , -0.51587 , -0.17517 ,  0.94459 ,\n",
       "         0.12705 , -0.33031 ,  0.75951 ,  0.44449 ,  0.16553 , -0.19235 ,\n",
       "         0.065533, -0.12394 ,  0.61446 ,  0.89784 ,  0.17413 ,  0.41149 ,\n",
       "         1.191   , -0.39461 , -0.459   ,  0.022161, -0.50843 , -0.44464 ,\n",
       "         0.68721 , -0.7167  ,  0.20835 , -0.23437 ,  0.02604 , -0.47993 ,\n",
       "         0.31873 , -0.29135 ,  0.50273 , -0.55144 , -0.066692,  0.43873 ,\n",
       "        -0.24293 , -1.0247  ,  0.029375,  0.068499,  0.25451 , -1.9663  ,\n",
       "         0.26673 ,  0.88486 ], dtype=float32),\n",
       " array([-1.075    ,  0.18316  ,  0.32895  ,  0.63907  , -0.56016  ,\n",
       "         0.57065  , -1.6973   ,  0.23407  , -0.66     , -0.79543  ,\n",
       "        -0.87456  ,  0.47696  , -0.54104  , -0.010141 ,  0.33098  ,\n",
       "        -0.023072 ,  0.61555  ,  0.078931 , -0.26537  , -0.673    ,\n",
       "        -0.47385  ,  0.68288  ,  0.19332  , -0.35322  ,  0.38568  ,\n",
       "        -1.0739   , -0.72973  , -0.31692  ,  0.0067417, -0.01463  ,\n",
       "         1.5006   , -0.27884  , -0.31413  ,  0.071643 , -0.37914  ,\n",
       "        -0.11792  ,  0.25749  , -0.0093028,  0.52836  , -0.49904  ,\n",
       "         0.37546  ,  0.85248  , -1.2442   , -0.094374 ,  0.05246  ,\n",
       "         0.25983  , -0.98397  , -0.52276  , -0.77843  ,  1.8025   ],\n",
       "       dtype=float32),\n",
       " array([-0.53088  ,  0.13892  , -0.12795  , -0.094853 ,  0.26458  ,\n",
       "         0.66291  , -1.3818   ,  0.63771  , -0.64     , -0.36501  ,\n",
       "        -0.29955  ,  1.1933   ,  0.022851 ,  0.1158   ,  0.32931  ,\n",
       "        -0.11636  ,  0.27416  , -0.06985  , -0.6193   , -0.27823  ,\n",
       "        -0.040117 , -0.016558 ,  0.20754  ,  0.12619  ,  0.25587  ,\n",
       "        -0.93116  , -0.22679  , -0.14437  ,  0.28153  , -0.46689  ,\n",
       "         1.076    ,  0.35767  , -0.015097 ,  0.18035  ,  0.58771  ,\n",
       "         0.18644  , -0.22845  , -0.5673   ,  0.65721  , -0.32488  ,\n",
       "        -0.20792  ,  1.5596   , -1.5163   , -1.1573   ,  0.13157  ,\n",
       "         0.0086368,  0.14757  , -0.4774   , -0.44085  ,  1.5243   ],\n",
       "       dtype=float32),\n",
       " array([-0.33246  ,  0.96743  ,  0.12959  , -0.20149  ,  0.29269  ,\n",
       "        -0.42241  , -1.2442   ,  0.20015  , -0.53863  , -0.35508  ,\n",
       "        -0.44494  ,  0.57979  , -1.3057   ,  0.035685 ,  0.7738   ,\n",
       "         0.0035736,  1.1445   , -1.2053   , -0.48096  , -0.19235  ,\n",
       "        -0.39899  ,  0.42011  , -0.19906  , -0.014857 , -0.34018  ,\n",
       "        -0.94543  ,  0.72259  , -0.56788  , -0.85878  ,  0.29182  ,\n",
       "         2.0714   , -1.4323   ,  0.18008  , -0.74774  ,  0.015479 ,\n",
       "        -0.31071  ,  0.11831  ,  0.27912  ,  0.85021  , -0.63294  ,\n",
       "         1.0131   ,  0.99396  , -0.29473  , -0.61603  ,  0.55052  ,\n",
       "        -0.28938  , -0.10936  , -0.72139  , -0.65306  ,  1.3483   ],\n",
       "       dtype=float32),\n",
       " array([ 0.3691   ,  0.95701  , -0.83785  , -0.2143   ,  0.034267 ,\n",
       "         1.4412   , -0.95594  ,  0.023781 , -0.069092 , -0.86075  ,\n",
       "        -0.089889 ,  1.4155   , -0.35297  , -0.23195  ,  0.68667  ,\n",
       "         0.027121 ,  0.15515  , -0.48405  ,  0.13971  , -0.0036475,\n",
       "         0.39878  ,  1.2105   ,  0.47873  ,  0.39818  ,  1.313    ,\n",
       "        -0.26559  ,  0.34741  , -0.29525  , -0.707    , -0.14209  ,\n",
       "         0.20828  ,  0.16988  , -0.035548 , -0.1355   , -0.31802  ,\n",
       "        -0.21994  , -0.41145  , -0.10145  ,  0.77866  , -1.0139   ,\n",
       "         0.31763  ,  0.11064  ,  0.30528  , -1.2731   ,  0.064314 ,\n",
       "         0.40395  ,  0.13639  , -1.1875   ,  0.61469  ,  0.19161  ],\n",
       "       dtype=float32),\n",
       " array([ 0.1444  , -0.39781 , -0.30214 ,  0.79353 ,  0.23563 ,  0.56318 ,\n",
       "        -0.35814 ,  0.091786,  0.17833 , -0.2833  ,  0.25527 ,  0.51832 ,\n",
       "        -0.032017, -0.16956 ,  0.44499 ,  0.48365 , -0.1729  , -0.082584,\n",
       "         0.32197 , -0.10439 , -0.15957 ,  0.89328 ,  0.23489 ,  0.37262 ,\n",
       "         0.99256 ,  0.25818 ,  0.14352 , -0.3861  , -0.65023 , -0.040338,\n",
       "        -0.13554 , -0.09983 , -0.031266,  0.32168 , -0.095695, -0.24858 ,\n",
       "        -0.2932  , -0.11367 ,  0.14484 , -1.2037  ,  0.26535 ,  0.32956 ,\n",
       "        -0.4638  , -0.94014 ,  0.51861 ,  0.038514,  0.25159 , -1.5326  ,\n",
       "         0.037051,  0.50138 ], dtype=float32),\n",
       " array([-0.3811  ,  1.1439  ,  0.22862 , -0.69739 ,  0.068113,  0.51787 ,\n",
       "        -1.2119  , -0.076324, -1.1163  , -0.84702 , -1.0995  ,  1.2665  ,\n",
       "        -0.31665 , -0.63508 , -0.07403 ,  0.22557 ,  0.22987 , -0.33641 ,\n",
       "        -0.59308 , -0.26622 ,  0.44539 ,  0.23679 , -0.32833 , -1.0097  ,\n",
       "         0.57147 , -2.0506  ,  0.13884 , -0.52393 , -0.30766 ,  0.22662 ,\n",
       "         1.4254  , -0.47093 , -0.44459 , -0.71749 ,  0.60025 , -0.25851 ,\n",
       "         0.26066 ,  0.14437 , -0.49375 , -0.39191 ,  0.030213,  0.87374 ,\n",
       "        -0.41465 , -1.0149  ,  0.28665 ,  0.85734 , -0.89267 , -0.59213 ,\n",
       "        -0.24017 ,  0.77824 ], dtype=float32),\n",
       " array([-0.51178 ,  0.83695 , -0.097588, -0.088469,  0.33524 ,  1.062   ,\n",
       "        -0.78525 ,  0.015852, -0.08664 , -0.30108 ,  0.046058,  0.62862 ,\n",
       "        -0.34086 , -0.22738 ,  0.97905 ,  0.58733 ,  0.062205, -0.11223 ,\n",
       "        -0.048274,  0.45908 ,  0.59726 ,  0.93911 ,  0.21475 ,  0.40839 ,\n",
       "         0.89855 , -0.44329 , -0.013826, -0.49728 ,  0.36541 , -0.60202 ,\n",
       "        -0.15952 , -0.25162 ,  0.11619 , -0.23637 ,  0.10637 , -0.2369  ,\n",
       "        -0.011738, -0.19625 ,  0.066112, -0.25777 , -0.052363,  0.2484  ,\n",
       "        -0.024328, -0.87035 ,  0.42756 , -0.10316 ,  0.34086 , -1.3397  ,\n",
       "        -0.58994 ,  0.75355 ], dtype=float32),\n",
       " array([ 1.3151  ,  0.63278 , -0.93114 , -0.28505 , -0.18065 ,  1.0446  ,\n",
       "        -0.17295 ,  0.40703 , -0.50872 , -0.014145,  0.24439 ,  1.108   ,\n",
       "        -0.036887, -1.5811  ,  0.74677 , -0.10979 , -0.58505 , -0.35141 ,\n",
       "         0.28188 ,  0.56661 , -0.31382 ,  0.15652 , -0.85317 ,  0.22385 ,\n",
       "         0.78731 , -0.61758 ,  0.19053 , -0.48083 , -0.17316 , -0.44311 ,\n",
       "        -0.26529 ,  0.14244 ,  0.078697,  0.14949 ,  0.37072 ,  0.026348,\n",
       "        -0.075227,  0.47352 , -0.31374 , -0.18299 ,  0.32294 ,  0.38887 ,\n",
       "         0.54579 , -0.99759 , -0.82511 , -0.041572, -0.38632 , -2.3365  ,\n",
       "         0.22655 , -0.20828 ], dtype=float32),\n",
       " array([-8.8123e-01,  8.2891e-01,  3.7694e-01,  4.4833e-01,  1.7482e-01,\n",
       "         1.1751e+00, -1.1878e+00,  1.5700e-01, -6.1533e-01, -6.4032e-01,\n",
       "        -5.4092e-01,  1.4348e+00, -5.5619e-01,  3.2093e-01,  5.3486e-01,\n",
       "         8.3639e-02,  5.6714e-01,  6.9777e-01, -5.9983e-01, -5.1282e-01,\n",
       "         4.1840e-01,  5.6947e-01,  6.0340e-01,  2.3695e-01,  3.4830e-01,\n",
       "        -1.3877e+00,  1.5486e-01, -6.3203e-02, -4.8972e-04, -2.3702e-01,\n",
       "         1.2318e+00, -7.8970e-01, -2.2067e-01, -1.0560e-01,  3.4888e-01,\n",
       "         1.2663e-01, -3.1450e-01, -3.6210e-01,  6.1272e-01, -8.3193e-01,\n",
       "         2.0834e-01,  9.4545e-01, -8.0578e-01, -5.7852e-01,  5.9549e-01,\n",
       "         1.7848e-01, -2.1407e-01, -6.3185e-01, -1.9556e-01,  7.9653e-01],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_book = [book.search(word.lower()) for word in names]\n",
    "name_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = np.stack(name_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alice</th>\n",
       "      <th>Bob</th>\n",
       "      <th>Charlie</th>\n",
       "      <th>David</th>\n",
       "      <th>Emma</th>\n",
       "      <th>Fiona</th>\n",
       "      <th>George</th>\n",
       "      <th>Hannah</th>\n",
       "      <th>Isabella</th>\n",
       "      <th>Jack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446056</td>\n",
       "      <td>0.540106</td>\n",
       "      <td>0.469889</td>\n",
       "      <td>0.804588</td>\n",
       "      <td>0.704335</td>\n",
       "      <td>0.509538</td>\n",
       "      <td>0.795454</td>\n",
       "      <td>0.547919</td>\n",
       "      <td>0.605657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>0.446056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729476</td>\n",
       "      <td>0.644884</td>\n",
       "      <td>0.321230</td>\n",
       "      <td>0.312639</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.410920</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>0.764855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlie</th>\n",
       "      <td>0.540106</td>\n",
       "      <td>0.729476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536244</td>\n",
       "      <td>0.419993</td>\n",
       "      <td>0.373742</td>\n",
       "      <td>0.615513</td>\n",
       "      <td>0.527851</td>\n",
       "      <td>0.187277</td>\n",
       "      <td>0.781151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>David</th>\n",
       "      <td>0.469889</td>\n",
       "      <td>0.644884</td>\n",
       "      <td>0.536244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.385114</td>\n",
       "      <td>0.264023</td>\n",
       "      <td>0.631671</td>\n",
       "      <td>0.402353</td>\n",
       "      <td>0.130685</td>\n",
       "      <td>0.638008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emma</th>\n",
       "      <td>0.804588</td>\n",
       "      <td>0.321230</td>\n",
       "      <td>0.419993</td>\n",
       "      <td>0.385114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696199</td>\n",
       "      <td>0.452665</td>\n",
       "      <td>0.705395</td>\n",
       "      <td>0.585676</td>\n",
       "      <td>0.560839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiona</th>\n",
       "      <td>0.704335</td>\n",
       "      <td>0.312639</td>\n",
       "      <td>0.373742</td>\n",
       "      <td>0.264023</td>\n",
       "      <td>0.696199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170604</td>\n",
       "      <td>0.621130</td>\n",
       "      <td>0.457493</td>\n",
       "      <td>0.416530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George</th>\n",
       "      <td>0.509538</td>\n",
       "      <td>0.659827</td>\n",
       "      <td>0.615513</td>\n",
       "      <td>0.631671</td>\n",
       "      <td>0.452665</td>\n",
       "      <td>0.170604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461869</td>\n",
       "      <td>0.328048</td>\n",
       "      <td>0.708290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hannah</th>\n",
       "      <td>0.795454</td>\n",
       "      <td>0.410920</td>\n",
       "      <td>0.527851</td>\n",
       "      <td>0.402353</td>\n",
       "      <td>0.705395</td>\n",
       "      <td>0.621130</td>\n",
       "      <td>0.461869</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481333</td>\n",
       "      <td>0.621056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Isabella</th>\n",
       "      <td>0.547919</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>0.187277</td>\n",
       "      <td>0.130685</td>\n",
       "      <td>0.585676</td>\n",
       "      <td>0.457493</td>\n",
       "      <td>0.328048</td>\n",
       "      <td>0.481333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jack</th>\n",
       "      <td>0.605657</td>\n",
       "      <td>0.764855</td>\n",
       "      <td>0.781151</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>0.560839</td>\n",
       "      <td>0.416530</td>\n",
       "      <td>0.708290</td>\n",
       "      <td>0.621056</td>\n",
       "      <td>0.131430</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Alice       Bob   Charlie     David      Emma     Fiona  \\\n",
       "Alice     1.000000  0.446056  0.540106  0.469889  0.804588  0.704335   \n",
       "Bob       0.446056  1.000000  0.729476  0.644884  0.321230  0.312639   \n",
       "Charlie   0.540106  0.729476  1.000000  0.536244  0.419993  0.373742   \n",
       "David     0.469889  0.644884  0.536244  1.000000  0.385114  0.264023   \n",
       "Emma      0.804588  0.321230  0.419993  0.385114  1.000000  0.696199   \n",
       "Fiona     0.704335  0.312639  0.373742  0.264023  0.696199  1.000000   \n",
       "George    0.509538  0.659827  0.615513  0.631671  0.452665  0.170604   \n",
       "Hannah    0.795454  0.410920  0.527851  0.402353  0.705395  0.621130   \n",
       "Isabella  0.547919  0.026504  0.187277  0.130685  0.585676  0.457493   \n",
       "Jack      0.605657  0.764855  0.781151  0.638008  0.560839  0.416530   \n",
       "\n",
       "            George    Hannah  Isabella      Jack  \n",
       "Alice     0.509538  0.795454  0.547919  0.605657  \n",
       "Bob       0.659827  0.410920  0.026504  0.764855  \n",
       "Charlie   0.615513  0.527851  0.187277  0.781151  \n",
       "David     0.631671  0.402353  0.130685  0.638008  \n",
       "Emma      0.452665  0.705395  0.585676  0.560839  \n",
       "Fiona     0.170604  0.621130  0.457493  0.416530  \n",
       "George    1.000000  0.461869  0.328048  0.708290  \n",
       "Hannah    0.461869  1.000000  0.481333  0.621056  \n",
       "Isabella  0.328048  0.481333  1.000000  0.131430  \n",
       "Jack      0.708290  0.621056  0.131430  1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(people, people), columns=names, index=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_app = Profiles(names=names, embedding=people)\n",
    "profile_app.location.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Alice       Bob   Charlie     David      Emma     Fiona  \\\n",
      "Alice     1.000000  0.446056  0.540106  0.469889  0.804588  0.704335   \n",
      "Bob       0.446056  1.000000  0.729476  0.644884  0.321230  0.312639   \n",
      "Charlie   0.540106  0.729476  1.000000  0.536244  0.419993  0.373742   \n",
      "David     0.469889  0.644884  0.536244  1.000000  0.385114  0.264023   \n",
      "Emma      0.804588  0.321230  0.419993  0.385114  1.000000  0.696199   \n",
      "Fiona     0.704335  0.312639  0.373742  0.264023  0.696199  1.000000   \n",
      "George    0.509538  0.659827  0.615513  0.631671  0.452665  0.170604   \n",
      "Hannah    0.795454  0.410920  0.527851  0.402353  0.705395  0.621130   \n",
      "Isabella  0.547919  0.026504  0.187277  0.130685  0.585676  0.457493   \n",
      "Jack      0.605657  0.764855  0.781151  0.638008  0.560839  0.416530   \n",
      "\n",
      "            George    Hannah  Isabella      Jack  \n",
      "Alice     0.509538  0.795454  0.547919  0.605657  \n",
      "Bob       0.659827  0.410920  0.026504  0.764855  \n",
      "Charlie   0.615513  0.527851  0.187277  0.781151  \n",
      "David     0.631671  0.402353  0.130685  0.638008  \n",
      "Emma      0.452665  0.705395  0.585676  0.560839  \n",
      "Fiona     0.170604  0.621130  0.457493  0.416530  \n",
      "George    1.000000  0.461869  0.328048  0.708290  \n",
      "Hannah    0.461869  1.000000  0.481333  0.621056  \n",
      "Isabella  0.328048  0.481333  1.000000  0.131430  \n",
      "Jack      0.708290  0.621056  0.131430  1.000000  \n"
     ]
    }
   ],
   "source": [
    "profile_app.tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 \t loss 22.214220176684307\n",
      "EPOCH 1 \t loss 10.68148720855603\n",
      "EPOCH 2 \t loss 6.857888446926399\n",
      "EPOCH 3 \t loss 5.323384460519429\n",
      "EPOCH 4 \t loss 4.520614143451544\n",
      "EPOCH 5 \t loss 3.990100361587179\n",
      "EPOCH 6 \t loss 3.587378390461643\n",
      "EPOCH 7 \t loss 3.2605692501158132\n",
      "EPOCH 8 \t loss 2.986626982461855\n",
      "EPOCH 9 \t loss 2.7526526622685856\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomModel:\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int, loss_function, **kwargs: dict):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, hidden_dim)\n",
    "        self.biases = np.random.randn(hidden_dim)\n",
    "        self.w2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.b2 = np.random.randn(output_dim)\n",
    "        self.criterion = loss_function \n",
    "        self.learning_rate = 0.05 if 'lr' not in kwargs else kwargs['lr']\n",
    "        \n",
    "    def _weight_step(self, weights, biases, inputs, targets, output):\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1)\n",
    "        grad_output = 2 * (output - targets) / output.shape[0]\n",
    "        grad_weights = np.dot(inputs.T, grad_output)\n",
    "        grad_biases = np.sum(grad_output, axis=0)\n",
    "        weights -= self.learning_rate * grad_weights \n",
    "        biases -= self.learning_rate * grad_biases \n",
    "        \n",
    "    def step(self, inputs, targets):\n",
    "        assert hasattr(self, 'max_epoch'), 'No max epoch attr found. Config max epoch attr before training...'\n",
    "        outputs = self.forward(inputs)\n",
    "        output = np.dot(outputs, self.w2) + self.b2\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        loss = self.criterion(output, targets)\n",
    "        self._weight_step(self.weights, self.biases, inputs, targets, output)\n",
    "        self._weight_step(self.w2, self.b2, inputs, targets, output)\n",
    "        print(f\"EPOCH {self.max_epoch[0]} \\t loss {loss}\")\n",
    "        self.max_epoch = self.max_epoch[1:]\n",
    "        return self.step(inputs, targets) if len(self.max_epoch) != 0 else None \n",
    "    \n",
    "    def forward(self, inputs):  \n",
    "        assert inputs.shape[-1] == 50 and len(inputs.shape) == 3, f\"Expected input shape (batch_size, 1, 50) but received input shape {inputs.shape}\"\n",
    "        x = np.dot(inputs, self.weights) + self.biases # [10, 1, hidden_dim]\n",
    "        assert x.shape[0] == inputs.shape[0], f\"Expected 1st dim shape of x to be {inputs.shape} but received: {x.shape}\"\n",
    "        return x\n",
    "    \n",
    "    def update(self, batch_input: np.ndarray, batch_target: np.ndarray, max_epoch: np.ndarray):\n",
    "        self.max_epoch = list(range(max_epoch))\n",
    "        self.step(batch_input, batch_target)\n",
    "        \n",
    "def mse_loss(predictions: np.ndarray, targets: np.ndarray) -> float:\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def target_location(hit_list: list, target_list: list, kill_list: list, x: int, y: int):\n",
    "    assert len(target_list) == len(kill_list)\n",
    "    assert len(hit_list) == len(target_list)\n",
    "\n",
    "    grid = np.zeros((x, y))\n",
    "    for index in range(len(hit_list)):\n",
    "        hits = hit_list[index]\n",
    "        target = target_list[index]\n",
    "        kills = kill_list[index]\n",
    "        if x > 1 and y > 1:\n",
    "            grid[hits, target] = 1\n",
    "            grid[hits, kills] = -1\n",
    "            grid[target, hits] = 1\n",
    "            grid[kills, hits] = -1 \n",
    "        else: \n",
    "            assert x != y, f\"this would mean input shape (x, y) == (1, 1) which is invalid\"\n",
    "            if x == 1 and y > 1:\n",
    "                grid[:, hits] = 1\n",
    "                grid[:, kills] = -1\n",
    "            elif x > 1 and y == 1:\n",
    "                grid[hits, :] = 1\n",
    "                grid[kills, :] = -1\n",
    "            \n",
    "    return grid \n",
    "\n",
    "def evaluate_model(model: CustomModel, entity_embedding: np.ndarray, threshold: int = 0.5, name_tags: list = None):\n",
    "    prev_mat = cosine_similarity(entity_embedding, entity_embedding)\n",
    "    output = [model.forward(i) for i in entity_embedding]\n",
    "    output = np.stack(output) # [10, 30]?!\n",
    "    #assert output.shape == entity_embedding.shape, f\"Output shape: {output.shape} Embedding shape: {entity_embedding.shape}\"\n",
    "    new_mat = cosine_similarity(output, output)\n",
    "    diff_mat = prev_mat - new_mat \n",
    "    \n",
    "    assert new_mat.shape == prev_mat.shape \n",
    "    if name_tags:\n",
    "        import pandas as pd \n",
    "        prev_df = pd.DataFrame(np.round(prev_mat, 5), columns=name_tags, index=name_tags)\n",
    "        new_df = pd.DataFrame(np.round(new_mat, 5), columns=name_tags, index=name_tags)\n",
    "        #diff_df = pd.DataFrame(diff_mat, columns=name_tags, index=name_tags)\n",
    "        print(prev_df)\n",
    "        print(new_df)\n",
    "    else:\n",
    "        print(np.round(prev_mat), 5)\n",
    "        print(np.round(new_mat), 5)\n",
    "    print(np.where(diff_mat > threshold, 1, 0))\n",
    "    \n",
    "model = CustomModel(50, 1, 30, mse_loss)\n",
    "model.update(batch_input=profile_app.location, batch_target=np.ones((10, 1)), max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mean: 1.0649443524286095',\n",
       " 'Mean: 0.8311697866211062',\n",
       " 'Mean: 1.5073113730794947',\n",
       " 'Mean: 1.4912497850036057',\n",
       " 'Mean: 0.6693361137691792',\n",
       " 'Mean: 0.6065770308561704',\n",
       " 'Mean: 0.6699219604907855',\n",
       " 'Mean: 0.9044316598610441',\n",
       " 'Mean: 1.398800954665618',\n",
       " 'Mean: 0.8264205221334359']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"Mean: {model.forward(i).mean()}\" for i in profile_app.location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [model.forward(i) for i in profile_app.location] \n",
    "np.stack(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]] 5\n",
      "[[ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0. -0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0. -0.  0.  0.  0.]\n",
      " [ 0.  0. -0.  0.  1.  0. -0.  0. -0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1. -0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -0. -0. -0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]] 5\n",
      "[[-0.   -0.12  0.12  0.07  0.56  0.5   0.44  0.5   0.2   0.59]\n",
      " [-0.12  0.    0.49  0.41  0.13 -0.11  0.49  0.11 -0.27  0.57]\n",
      " [ 0.12  0.49 -0.    0.19  0.49  0.3   0.47  0.3  -0.3   0.73]\n",
      " [ 0.07  0.41  0.19 -0.    0.    0.02  0.67  0.08 -0.24  0.43]\n",
      " [ 0.56  0.13  0.49  0.   -0.    0.44  0.6   0.59  0.65  0.13]\n",
      " [ 0.5  -0.11  0.3   0.02  0.44 -0.    0.32  0.46  0.34  0.33]\n",
      " [ 0.44  0.49  0.47  0.67  0.6   0.32  0.    0.15  0.    0.65]\n",
      " [ 0.5   0.11  0.3   0.08  0.59  0.46  0.15 -0.    0.23  0.33]\n",
      " [ 0.2  -0.27 -0.3  -0.24  0.65  0.34  0.    0.23  0.    0.03]\n",
      " [ 0.59  0.57  0.73  0.43  0.13  0.33  0.65  0.33  0.03  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model=model, entity_embedding=profile_app.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trialing out with a actual target vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination on constructing the target vector\n",
    "1.\thit_list: These are the indices of entity embeddings you want to change (modify the embeddings).\n",
    "2.\ttarget_list: These are the indices of the entity embeddings you want the hit_list entities to be closer to.\n",
    "3.\tkill_list: These are the indices of the entity embeddings you want the hit_list entities to be farther from.\n",
    "\n",
    "Ideally the objective loss function would be able to:\n",
    "\n",
    "1.\tMinimizes the distance between the embeddings in the hit_list and the target_list.\n",
    "2.\tMaximizes the distance between the embeddings in the hit_list and the kill_list.\n",
    "\n",
    "However, for the sake of simplifying this experiment and my curiousity on other things, I have used MSE as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 \t loss 24.784918799453397\n",
      "EPOCH 1 \t loss 11.338007865873994\n",
      "EPOCH 2 \t loss 7.060437121456583\n",
      "EPOCH 3 \t loss 5.462876413166307\n",
      "EPOCH 4 \t loss 4.691157749288646\n",
      "EPOCH 5 \t loss 4.20593135870951\n",
      "EPOCH 6 \t loss 3.8431567587684823\n",
      "EPOCH 7 \t loss 3.54767039975097\n",
      "EPOCH 8 \t loss 3.2971879670920257\n",
      "EPOCH 9 \t loss 3.0802994211231636\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(50, 1, 30, mse_loss)\n",
    "model.update(batch_input=profile_app.location, batch_target=target_location(hit_list=[1, 2, 3], target_list=[4, 5, 6], kill_list=[7, 8, 9], x=10, y=1), max_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Alice      Bob  Charlie    David     Emma    Fiona   George  \\\n",
      "Alice     1.00000  0.44606  0.54011  0.46989  0.80459  0.70434  0.50954   \n",
      "Bob       0.44606  1.00000  0.72948  0.64488  0.32123  0.31264  0.65983   \n",
      "Charlie   0.54011  0.72948  1.00000  0.53624  0.41999  0.37374  0.61551   \n",
      "David     0.46989  0.64488  0.53624  1.00000  0.38511  0.26402  0.63167   \n",
      "Emma      0.80459  0.32123  0.41999  0.38511  1.00000  0.69620  0.45266   \n",
      "Fiona     0.70434  0.31264  0.37374  0.26402  0.69620  1.00000  0.17060   \n",
      "George    0.50954  0.65983  0.61551  0.63167  0.45266  0.17060  1.00000   \n",
      "Hannah    0.79545  0.41092  0.52785  0.40235  0.70540  0.62113  0.46187   \n",
      "Isabella  0.54792  0.02650  0.18728  0.13069  0.58568  0.45749  0.32805   \n",
      "Jack      0.60566  0.76485  0.78115  0.63801  0.56084  0.41653  0.70829   \n",
      "\n",
      "           Hannah  Isabella     Jack  \n",
      "Alice     0.79545   0.54792  0.60566  \n",
      "Bob       0.41092   0.02650  0.76485  \n",
      "Charlie   0.52785   0.18728  0.78115  \n",
      "David     0.40235   0.13069  0.63801  \n",
      "Emma      0.70540   0.58568  0.56084  \n",
      "Fiona     0.62113   0.45749  0.41653  \n",
      "George    0.46187   0.32805  0.70829  \n",
      "Hannah    1.00000   0.48133  0.62106  \n",
      "Isabella  0.48133   1.00000  0.13143  \n",
      "Jack      0.62106   0.13143  1.00000  \n",
      "            Alice      Bob  Charlie    David     Emma    Fiona   George  \\\n",
      "Alice     1.00000 -0.32312  0.13124 -0.00610 -0.03159 -0.26877 -0.23632   \n",
      "Bob      -0.32312  1.00000 -0.10311 -0.02093 -0.19737  0.16464 -0.17238   \n",
      "Charlie   0.13124 -0.10311  1.00000 -0.09411 -0.41496  0.13459 -0.04586   \n",
      "David    -0.00610 -0.02093 -0.09411  1.00000 -0.11354 -0.14375 -0.42471   \n",
      "Emma     -0.03159 -0.19737 -0.41496 -0.11354  1.00000 -0.31251 -0.13390   \n",
      "Fiona    -0.26877  0.16464  0.13459 -0.14375 -0.31251  1.00000 -0.05578   \n",
      "George   -0.23632 -0.17238 -0.04586 -0.42471 -0.13390 -0.05578  1.00000   \n",
      "Hannah    0.42713 -0.45800  0.02446 -0.25735  0.14598 -0.22172 -0.05137   \n",
      "Isabella -0.04537  0.20650 -0.19763 -0.06268 -0.31246  0.24933  0.04458   \n",
      "Jack     -0.38710  0.06389 -0.13129 -0.09473  0.01200 -0.09060 -0.19193   \n",
      "\n",
      "           Hannah  Isabella     Jack  \n",
      "Alice     0.42713  -0.04537 -0.38710  \n",
      "Bob      -0.45800   0.20650  0.06389  \n",
      "Charlie   0.02446  -0.19763 -0.13129  \n",
      "David    -0.25735  -0.06268 -0.09473  \n",
      "Emma      0.14598  -0.31246  0.01200  \n",
      "Fiona    -0.22172   0.24933 -0.09060  \n",
      "George   -0.05137   0.04458 -0.19193  \n",
      "Hannah    1.00000  -0.29637 -0.14551  \n",
      "Isabella -0.29637   1.00000 -0.23601  \n",
      "Jack     -0.14551  -0.23601  1.00000  \n",
      "[[0 1 0 0 1 1 1 0 1 1]\n",
      " [1 0 1 1 1 0 1 1 0 1]\n",
      " [0 1 0 1 1 0 1 1 0 1]\n",
      " [0 1 1 0 0 0 1 1 0 1]\n",
      " [1 1 1 0 0 1 1 1 1 1]\n",
      " [1 0 0 0 1 0 0 1 0 1]\n",
      " [1 1 1 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 1 1 0 1 1]\n",
      " [1 0 0 0 1 0 0 1 0 0]\n",
      " [1 1 1 1 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, entity_embedding=profile_app.location, name_tags=profile_app.username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garfield",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
